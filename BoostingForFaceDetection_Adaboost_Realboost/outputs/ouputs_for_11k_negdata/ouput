/home/vaishnaviravindran29/PycharmProjects/prml/venv/bin/python /home/vaishnaviravindran29/PycharmProjects/prml/main.py
Load in 23838 images, 11838 faces, 12000 non-faces
Calcuate activations for 10032 weak classifiers, using 23838 imags.
shape of data is (23838, 16, 16)
Weak Classifier No. 100 has finished applying
Weak Classifier No. 200 has finished applying
Weak Classifier No. 300 has finished applying
Weak Classifier No. 400 has finished applying
Weak Classifier No. 500 has finished applying
Weak Classifier No. 600 has finished applying
Weak Classifier No. 700 has finished applying
Weak Classifier No. 800 has finished applying
Weak Classifier No. 900 has finished applying
Weak Classifier No. 1000 has finished applying
Weak Classifier No. 1100 has finished applying
Weak Classifier No. 1200 has finished applying
Weak Classifier No. 1300 has finished applying
Weak Classifier No. 1400 has finished applying
Weak Classifier No. 1500 has finished applying
Weak Classifier No. 1600 has finished applying
Weak Classifier No. 1700 has finished applying
Weak Classifier No. 1800 has finished applying
Weak Classifier No. 1900 has finished applying
Weak Classifier No. 2000 has finished applying
Weak Classifier No. 2100 has finished applying
Weak Classifier No. 2200 has finished applying
Weak Classifier No. 2300 has finished applying
Weak Classifier No. 2400 has finished applying
Weak Classifier No. 2500 has finished applying
Weak Classifier No. 2600 has finished applying
Weak Classifier No. 2700 has finished applying
Weak Classifier No. 2800 has finished applying
Weak Classifier No. 2900 has finished applying
Weak Classifier No. 3000 has finished applying
Weak Classifier No. 3100 has finished applying
Weak Classifier No. 3200 has finished applying
Weak Classifier No. 3300 has finished applying
Weak Classifier No. 3400 has finished applying
Weak Classifier No. 3500 has finished applying
Weak Classifier No. 3600 has finished applying
Weak Classifier No. 3700 has finished applying
Weak Classifier No. 3800 has finished applying
Weak Classifier No. 3900 has finished applying
Weak Classifier No. 4000 has finished applying
Weak Classifier No. 4100 has finished applying
Weak Classifier No. 4200 has finished applying
Weak Classifier No. 4300 has finished applying
Weak Classifier No. 4400 has finished applying
Weak Classifier No. 4500 has finished applying
Weak Classifier No. 4600 has finished applying
Weak Classifier No. 4700 has finished applying
Weak Classifier No. 4800 has finished applying
Weak Classifier No. 4900 has finished applying
Weak Classifier No. 5000 has finished applying
Weak Classifier No. 5100 has finished applying
Weak Classifier No. 5200 has finished applying
Weak Classifier No. 5300 has finished applying
Weak Classifier No. 5400 has finished applying
Weak Classifier No. 5500 has finished applying
Weak Classifier No. 5600 has finished applying
Weak Classifier No. 5700 has finished applying
Weak Classifier No. 5800 has finished applying
Weak Classifier No. 5900 has finished applying
Weak Classifier No. 6000 has finished applying
Weak Classifier No. 6100 has finished applying
Weak Classifier No. 6200 has finished applying
Weak Classifier No. 6300 has finished applying
Weak Classifier No. 6400 has finished applying
Weak Classifier No. 6500 has finished applying
Weak Classifier No. 6600 has finished applying
Weak Classifier No. 6700 has finished applying
Weak Classifier No. 6800 has finished applying
Weak Classifier No. 6900 has finished applying
Weak Classifier No. 7000 has finished applying
Weak Classifier No. 7100 has finished applying
Weak Classifier No. 7200 has finished applying
Weak Classifier No. 7300 has finished applying
Weak Classifier No. 7400 has finished applying
Weak Classifier No. 7500 has finished applying
Weak Classifier No. 7600 has finished applying
Weak Classifier No. 7700 has finished applying
Weak Classifier No. 7800 has finished applying
Weak Classifier No. 7900 has finished applying
Weak Classifier No. 8000 has finished applying
Weak Classifier No. 8100 has finished applying
Weak Classifier No. 8200 has finished applying
Weak Classifier No. 8300 has finished applying
Weak Classifier No. 8400 has finished applying
Weak Classifier No. 8500 has finished applying
Weak Classifier No. 8600 has finished applying
Weak Classifier No. 8700 has finished applying
Weak Classifier No. 8800 has finished applying
Weak Classifier No. 8900 has finished applying
Weak Classifier No. 9000 has finished applying
Weak Classifier No. 9100 has finished applying
Weak Classifier No. 9200 has finished applying
Weak Classifier No. 9300 has finished applying
Weak Classifier No. 9400 has finished applying
Weak Classifier No. 9500 has finished applying
Weak Classifier No. 9600 has finished applying
Weak Classifier No. 9700 has finished applying
Weak Classifier No. 9800 has finished applying
Weak Classifier No. 9900 has finished applying
Weak Classifier No. 10000 has finished applying
Writing results to disk...
[Saved calculated activations to wc_activations.npy]
wc.activations shape is (10032, 23838)
wc.activations shape is (23838,)
194.251475 seconds for activation calculation
Performing Adaboost..
min_error_of_wc 0.2917610537796792
wc_index 87
training error is:  0.29176105377967954
108.271345 seconds for one epoch
np.sort(wc_errors[0:1000]) [0.29176105 0.29310345 0.29427804 0.29956372 0.30036077 0.30161926
 0.3049333  0.30522695 0.30539475 0.30770199 0.30879268 0.31248427
 0.31248427 0.31445591 0.3166373  0.31709875 0.31718265 0.3172246
 0.31995134 0.32091618 0.32150348 0.32171323 0.32175518 0.32217468
 0.32221663 0.32225858 0.32318147 0.32334927 0.32351707 0.32397852
 0.32439802 0.32464972 0.32477557 0.32485947 0.32502727 0.32540482
 0.32586626 0.32615991 0.32653746 0.3282574  0.32838325 0.32846715
 0.32846715 0.32888665 0.33102609 0.33106804 0.33115194 0.33119389
 0.33148754 0.33178119 0.33257824 0.33312358 0.33324943 0.33358503
 0.33446598 0.33463378 0.33463378 0.33551472 0.33555667 0.33576642
 0.33585032 0.33715077 0.33786391 0.33819951 0.33853511 0.33908046
 0.33975166 0.34176525 0.3421428  0.34285594 0.34310764 0.34327544
 0.34340129 0.34361104 0.34382079 0.34386274 0.34398859 0.34478564
 0.34495344 0.34587633 0.34625388 0.34654753 0.34692508 0.34797382
 0.34809967 0.34818357 0.34839332 0.34843527 0.34856112 0.34881282
 0.35141371 0.35145566 0.35166541 0.35233661 0.3525044  0.3530078
 0.35338535 0.35439215 0.35510529 0.35615404 0.35632184 0.35707694
 0.35732863 0.35741253 0.35762228 0.35858713 0.35888078 0.35892273
 0.35904858 0.35938418 0.36106217 0.36127192 0.36135582 0.36169142
 0.36194312 0.36240456 0.36336941 0.36353721 0.36420841 0.36462791
 0.36462791 0.36483765 0.3651313  0.36517325 0.36517325 0.3655508
 0.36584445 0.3659703  0.3660542  0.36609615 0.3663898  0.36651565
 0.3665576  0.3667254  0.3668932  0.36731269 0.36769024 0.36773219
 0.36806779 0.36823559 0.36886484 0.36907459 0.36941019 0.36957798
 0.36961993 0.36995553 0.37003943 0.37054283 0.37079453 0.37079453
 0.37079453 0.37087843 0.37100428 0.37100428 0.37129793 0.37138183
 0.37175938 0.37184328 0.37251447 0.37255642 0.37264032 0.37322762
 0.37360517 0.37377297 0.37394077 0.37436027 0.37477976 0.37486366
 0.37490561 0.37494756 0.37494756 0.37503146 0.37507341 0.37515731
 0.37540901 0.37540901 0.37574461 0.37578656 0.37595436 0.37603826
 0.37603826 0.37620606 0.37633191 0.37637386 0.37645776 0.37687725
 0.3770031  0.3771709  0.3775904  0.3775904  0.37763235 0.3776743
 0.37771625 0.37780015 0.37805185 0.37855525 0.3785972  0.37880695
 0.37880695 0.37943619 0.37947814 0.37972984 0.37972984 0.37989764
 0.38014934 0.38023324 0.38027519 0.38027519 0.38027519 0.38044299
 0.38056884 0.38069469 0.38073664 0.38077859 0.38077859 0.38103029
 0.38119809 0.38140784 0.38149174 0.38153369 0.38178538 0.38186928
 0.38186928 0.38191123 0.38203708 0.38224683 0.38258243 0.38262438
 0.38295998 0.38312778 0.38316973 0.38329558 0.38346338 0.38367313
 0.38392483 0.38405068 0.38405068 0.38421847 0.38447017 0.38463797
 0.38472187 0.38484772 0.38493162 0.38509942 0.38514137 0.38518332
 0.38530917 0.38535112 0.38556087 0.38560282 0.38572867 0.38572867
 0.38581257 0.38585452 0.38614817 0.38619012 0.38619012 0.38639987
 0.38644182 0.38656767 0.38673546 0.38715496 0.38749056 0.38761641
 0.38778421 0.38786811 0.38811981 0.38820371 0.38820371 0.38845541
 0.38845541 0.38845541 0.38849736 0.38887491 0.3891266  0.38916855
 0.38916855 0.3892105  0.38925245 0.38933635 0.38967195 0.3898817
 0.3899656  0.39000755 0.39025925 0.3906368  0.3908046  0.3909724
 0.3913919  0.39143385 0.39155969 0.39168554 0.39181139 0.39189529
 0.39197919 0.39197919 0.39206309 0.39235674 0.39248259 0.39273429
 0.39286014 0.39290209 0.39311184 0.39344744 0.39399278 0.39403473
 0.39432838 0.39441228 0.39478983 0.39483178 0.39516738 0.39529323
 0.39533518 0.39558688 0.39583858 0.39600638 0.39600638 0.39609028
 0.39609028 0.39621613 0.39634197 0.39701317 0.39768437 0.39772632
 0.39814582 0.39827167 0.39869117 0.39890091 0.39894286 0.39911066
 0.39911066 0.39919456 0.39927846 0.39932041 0.39948821 0.39969796
 0.39990771 0.40045306 0.40074671 0.40083061 0.40120816 0.401334
 0.401334   0.4014179  0.4018374  0.40187935 0.4026764  0.40288615
 0.403012   0.4030959  0.40313785 0.4031798  0.4032637  0.4033476
 0.40368319 0.40376709 0.40401879 0.40401879 0.40406074 0.40410269
 0.40448024 0.40448024 0.40464804 0.40498364 0.40498364 0.40523534
 0.40531924 0.40536119 0.40548704 0.40590654 0.40603239 0.40603239
 0.40653578 0.40674553 0.40687138 0.40712308 0.40720698 0.40729088
 0.40737478 0.40766843 0.40775233 0.40783623 0.40842353 0.40842353
 0.40854937 0.40880107 0.40901082 0.40901082 0.40943032 0.40947227
 0.40976592 0.40980787 0.40989177 0.41026932 0.41081467 0.41136001
 0.41169561 0.41177951 0.41186341 0.41198926 0.41211511 0.41215706
 0.41224096 0.41236681 0.41253461 0.41257656 0.41307996 0.41316386
 0.41333166 0.41337361 0.4135414  0.4137092  0.41375115 0.413877
 0.41391895 0.4141287  0.4141287  0.4146321  0.4146321  0.4146321
 0.4146321  0.41467405 0.4147999  0.41492575 0.4151355  0.41517745
 0.41517745 0.4152194  0.41551305 0.415555   0.41568085 0.41589059
 0.41610034 0.41618424 0.41622619 0.41635204 0.41639399 0.41643594
 0.41651984 0.41664569 0.41672959 0.41677154 0.41681349 0.41689739
 0.41693934 0.41714909 0.41719104 0.41727494 0.41740079 0.41765249
 0.41765249 0.41790419 0.41794614 0.41811394 0.41819784 0.41823979
 0.41832368 0.41836563 0.41844953 0.41865928 0.41870123 0.41933048
 0.41937243 0.41949828 0.41954023 0.41987583 0.41991778 0.41995973
 0.42021143 0.42063093 0.42063093 0.42079872 0.42100847 0.42121822
 0.42151187 0.42155382 0.42159577 0.42172162 0.42172162 0.42180552
 0.42188942 0.42201527 0.42205722 0.42205722 0.42230892 0.42272842
 0.42281232 0.42293817 0.42293817 0.42314792 0.42323181 0.42327376
 0.42335766 0.42344156 0.42348351 0.42402886 0.42411276 0.42411276
 0.42453226 0.42457421 0.42482591 0.42495176 0.42503566 0.42507761
 0.42532931 0.42537126 0.4256649  0.4259166  0.42646195 0.42671365
 0.4267556  0.42679755 0.4270073  0.4271751  0.42721705 0.427259
 0.4273429  0.4274268  0.42755265 0.42763655 0.42780435 0.4279302
 0.42822384 0.42830774 0.42839164 0.42839164 0.42847554 0.42855944
 0.42855944 0.42860139 0.42872724 0.42885309 0.42910479 0.42927259
 0.42939844 0.42948234 0.42973404 0.42977599 0.43027939 0.43040524
 0.43040524 0.43065693 0.43086668 0.43099253 0.43103448 0.43120228
 0.43157983 0.43178958 0.43220908 0.43279638 0.43309002 0.43346757
 0.43355147 0.43355147 0.43363537 0.43371927 0.43376122 0.43426462
 0.43430657 0.43447437 0.43451632 0.43451632 0.43476802 0.43476802
 0.43480997 0.43535532 0.43539726 0.43569091 0.43590066 0.43590066
 0.43590066 0.43615236 0.43627821 0.43632016 0.43644601 0.43715916
 0.43728501 0.43728501 0.43728501 0.43732696 0.43736891 0.43762061
 0.43778841 0.43791425 0.438124   0.4383757  0.4383757  0.4384596
 0.4385435  0.4386274  0.4387113  0.43875325 0.43892105 0.43925665
 0.4393825  0.43950835 0.4395503  0.439802   0.43984395 0.43992785
 0.44001175 0.44001175 0.44026344 0.44034734 0.44043124 0.44051514
 0.44055709 0.44055709 0.44085074 0.44093464 0.44093464 0.44122829
 0.44143804 0.44152194 0.44160584 0.44181559 0.44185754 0.44189949
 0.44189949 0.44215119 0.44219314 0.44231899 0.44240289 0.44240289
 0.44244484 0.44265459 0.44265459 0.44269653 0.44282238 0.44299018
 0.44311603 0.44315798 0.44324188 0.44349358 0.44366138 0.44370333
 0.44378723 0.44395503 0.44403893 0.44403893 0.44408088 0.44412283
 0.44416478 0.44420673 0.44429063 0.44441648 0.44445843 0.44445843
 0.44450038 0.44466818 0.44466818 0.44475208 0.44479403 0.44512962
 0.44512962 0.44538132 0.44542327 0.44550717 0.44575887 0.44580082
 0.44584277 0.44601057 0.44613642 0.44651397 0.44655592 0.44655592
 0.44659787 0.44663982 0.44668177 0.44676567 0.44701737 0.44705932
 0.44714322 0.44718517 0.44731102 0.44731102 0.44739492 0.44739492
 0.44743687 0.44747882 0.44752077 0.44802416 0.44806611 0.44806611
 0.44810806 0.44810806 0.44835976 0.44840171 0.44856951 0.44861146
 0.44869536 0.44873731 0.44877926 0.44886316 0.44886316 0.44890511
 0.44903096 0.44903096 0.44903096 0.44907291 0.44915681 0.44919876
 0.44932461 0.44953436 0.44961826 0.44961826 0.44970216 0.44970216
 0.44978606 0.44978606 0.44982801 0.44982801 0.44995386 0.4500797
 0.4500797  0.4501636  0.45037335 0.45045725 0.4504992  0.45054115
 0.4505831  0.450667   0.4507509  0.4507509  0.45087675 0.45087675
 0.45087675 0.4509187  0.45096065 0.45104455 0.4510865  0.45112845
 0.4512543  0.4513382  0.45146405 0.45146405 0.451506   0.45154795
 0.45163185 0.45171575 0.4517577  0.4518416  0.45188355 0.4520094
 0.4520094  0.45205135 0.4520933  0.45213525 0.4521772  0.45230305
 0.452345   0.45238695 0.45251279 0.45259669 0.45259669 0.45268059
 0.45268059 0.45272254 0.45276449 0.45280644 0.45284839 0.45289034
 0.45289034 0.45289034 0.45297424 0.45310009 0.45322594 0.45360349
 0.45372934 0.45372934 0.45377129 0.45377129 0.45381324 0.45385519
 0.45385519 0.45385519 0.45393909 0.45398104 0.45406494 0.45410689
 0.45414884 0.45423274 0.45423274 0.45427469 0.45452639 0.45452639
 0.45456834 0.45469419 0.45507173 0.45511368 0.45511368 0.45519758
 0.45523953 0.45523953 0.45528148 0.45528148 0.45528148 0.45528148
 0.45532343 0.45553318 0.45565903 0.45586878 0.45595268 0.45603658
 0.45620438 0.45624633 0.45641413 0.45645608 0.45645608 0.45645608
 0.45645608 0.45653998 0.45670778 0.45712728 0.45716923 0.45725313
 0.45733702 0.45763067 0.45771457 0.45784042 0.45788237 0.45792432
 0.45792432 0.45792432 0.45796627 0.45796627 0.45813407 0.45813407
 0.45817602 0.45842772 0.45859552 0.45880527 0.45880527 0.45888917
 0.45888917 0.45893112 0.45905697 0.45922477 0.45935062 0.45935062
 0.45939257 0.45939257 0.45947647 0.45947647 0.45960232 0.45968622
 0.45972817 0.45985401 0.45989596 0.46006376 0.46006376 0.46023156
 0.46031546 0.46048326 0.46048326 0.46052521 0.46052521 0.46056716
 0.46069301 0.46069301 0.46086081 0.46086081 0.46086081 0.46094471
 0.46102861 0.46107056 0.46115446 0.46115446 0.46115446 0.46128031
 0.46132226 0.46136421 0.46149006 0.46161591 0.46161591 0.46165786
 0.46178371 0.46186761 0.46186761 0.46186761 0.46190956 0.46195151
 0.46224515 0.4622871  0.46266465 0.46274855 0.4627905  0.46300025
 0.4630422  0.46308415 0.46341975 0.46350365 0.46367145 0.46367145
 0.46375535 0.46375535 0.4638812  0.46392315 0.4639651  0.4639651
 0.46409095 0.4641329  0.46417485 0.46417485 0.46442655 0.46451045
 0.46467824 0.46472019 0.46492994 0.46509774 0.46513969 0.46585284
 0.46597869 0.46614649 0.46627234 0.46631429 0.46639819 0.46644014
 0.46664989 0.46677574 0.46681769 0.46715328 0.46719523 0.46736303
 0.46748888 0.46761473 0.46765668 0.46790838 0.46803423 0.46824398
 0.46841178 0.46845373 0.46853763 0.46874738 0.46883128 0.46895713
 0.46904103 0.46920883 0.46925078 0.46929273 0.46950248 0.46950248
 0.47004782 0.47004782 0.47008977 0.47076097 0.47088682 0.47088682
 0.47092877 0.47113852 0.47113852 0.47164192 0.47172582 0.47172582
 0.47197751 0.47201946 0.47206141 0.47252286 0.47256481 0.47294236
 0.47315211 0.47319406 0.47323601 0.47344576 0.47373941 0.47386526
 0.47390721 0.47403306 0.47462035 0.4746623  0.4746623  0.4746623
 0.47495595 0.4755013  0.47554325 0.4758369  0.4760047  0.4760886
 0.4761725  0.47692759 0.47730514 0.47814414 0.47835389 0.47873144
 0.47877339 0.47881534 0.47885729 0.47885729 0.47906704 0.48049333
 0.48129038 0.48183572 0.48301032 0.48380737 0.48414297 0.48414297
 0.48418491 0.48473026 0.48494001 0.48577901]
Check training accuracy is:  0.7082389462203205
[Find cached wc_errors, chosen_wcs.pkl loading...]
9
min_error_of_wc 0.3354066230892261
wc_index 8892
training error is:  0.2912996056716167
88.863396 seconds for one epoch
min_error_of_wc 0.36167087352818317
wc_index 5383
training error is:  0.25312526218642506
88.124958 seconds for one epoch
min_error_of_wc 0.36040333971805505
wc_index 6201
training error is:  0.24884637972984314
87.531857 seconds for one epoch
min_error_of_wc 0.36888803753606736
wc_index 3429
training error is:  0.22585787398271662
89.406702 seconds for one epoch
min_error_of_wc 0.39011055563532454
wc_index 8400
training error is:  0.229801157815253
89.391598 seconds for one epoch
min_error_of_wc 0.40169560874333926
wc_index 2150
training error is:  0.21742595855356994
88.150982 seconds for one epoch
min_error_of_wc 0.393608516094995
wc_index 3412
training error is:  0.20882624381240034
88.829101 seconds for one epoch
min_error_of_wc 0.4095046728452592
wc_index 31
training error is:  0.2052185586039097
89.544824 seconds for one epoch
min_error_of_wc 0.41053218663020924
wc_index 116
training error is:  0.1990519338870711
88.962728 seconds for one epoch
np.sort(wc_errors[0:1000]) [0.41053219 0.41328647 0.41458889 0.41782174 0.41916798 0.4193365
 0.4195622  0.41978134 0.42040642 0.42040922 0.42040922 0.42040922
 0.42040922 0.42040922 0.42040922 0.42040922 0.42040922 0.42040922
 0.42040922 0.42040922 0.42040922 0.42040922 0.42040922 0.42040922
 0.42040922 0.42040922 0.42040922 0.42040922 0.42040922 0.42040922
 0.42040922 0.42040922 0.42040922 0.42040922 0.42040922 0.42041236
 0.42041509 0.42041561 0.42041561 0.42041561 0.42041561 0.42041561
 0.42041561 0.42041561 0.42041561 0.42041561 0.42041561 0.42041561
 0.42041561 0.42041561 0.42041561 0.42041561 0.42041561 0.42041561
 0.42041561 0.42041561 0.42041561 0.42041561 0.42041561 0.42041561
 0.42041561 0.42041561 0.42041561 0.42041561 0.42041561 0.42041561
 0.42041561 0.42041561 0.42041561 0.42041561 0.42041561 0.42041561
 0.42041561 0.42041561 0.42041561 0.42041561 0.42041561 0.42041561
 0.42041561 0.42041561 0.42041561 0.42041561 0.42041561 0.42041561
 0.42041561 0.42041561 0.42041561 0.42041561 0.42041688 0.42041688
 0.42041688 0.42041688 0.42041688 0.42041688 0.42041688 0.42041705
 0.42041728 0.42041728 0.42041728 0.42041838 0.42041838 0.42041838
 0.42041838 0.42041863 0.42041863 0.42041863 0.42041863 0.42041863
 0.42041863 0.42041863 0.42041863 0.42041863 0.42041863 0.42041863
 0.42041863 0.42041863 0.42041863 0.42041863 0.42041863 0.42041863
 0.42041863 0.42041863 0.42041863 0.42041863 0.42041863 0.42041863
 0.42041863 0.42041863 0.42041923 0.42041923 0.42041923 0.42041923
 0.42041923 0.42041923 0.42041923 0.42041923 0.42041923 0.42041923
 0.42041923 0.42041923 0.42041923 0.42041923 0.42041923 0.42041923
 0.42041923 0.42041923 0.42041923 0.42041923 0.42041923 0.42041923
 0.42041923 0.42041923 0.42041923 0.42041923 0.42041923 0.42041923
 0.42041923 0.42041923 0.42041923 0.42041923 0.42041923 0.42041923
 0.42041923 0.42041923 0.42041923 0.42041923 0.42041923 0.42041923
 0.42041923 0.42041923 0.42041923 0.42041923 0.42041923 0.42041923
 0.42041923 0.42041923 0.42041923 0.42041923 0.42041923 0.42041923
 0.42041923 0.42041923 0.42041923 0.42041923 0.42041923 0.42041923
 0.42041923 0.42041923 0.42041923 0.42041923 0.42041923 0.42041923
 0.42041923 0.42041923 0.42041923 0.42041937 0.42041937 0.42041937
 0.42041937 0.42041937 0.42041937 0.42041937 0.42041937 0.42041937
 0.42041937 0.42041937 0.42041937 0.42041937 0.42041937 0.42041937
 0.42041937 0.42041937 0.42041937 0.42041937 0.42041937 0.42041937
 0.42041937 0.42041937 0.42041937 0.42041937 0.42041937 0.42041937
 0.42041937 0.42041937 0.42041937 0.42041937 0.42042087 0.42042087
 0.42042087 0.42042087 0.42042087 0.42042087 0.42042087 0.42042087
 0.42042087 0.42042087 0.42042087 0.42042087 0.42042258 0.42042258
 0.42042258 0.42042258 0.42042258 0.42042258 0.42042275 0.42042275
 0.42042275 0.42042275 0.42042275 0.42042275 0.42042275 0.42042275
 0.42042275 0.42042275 0.42042275 0.42042275 0.42042275 0.42042275
 0.42042275 0.42042275 0.42042275 0.42042275 0.42042275 0.42042275
 0.42042275 0.42042275 0.42042275 0.42042275 0.42042275 0.42042275
 0.42042275 0.42042275 0.42042275 0.42042275 0.42042275 0.42042275
 0.42042275 0.42042275 0.42042275 0.42042343 0.42042343 0.42042343
 0.42042343 0.42042343 0.42042343 0.42042343 0.42042343 0.42042343
 0.42042343 0.42042958 0.42042958 0.42042958 0.42043055 0.42043323
 0.42043323 0.42043323 0.42043323 0.42043437 0.42043716 0.42043716
 0.42043716 0.42043716 0.42043716 0.42043716 0.42043716 0.42043716
 0.42043716 0.42043716 0.42043716 0.42043716 0.42043716 0.42043716
 0.42043716 0.42043716 0.42043716 0.42043716 0.42043716 0.42043716
 0.42043716 0.42043716 0.42043716 0.42043716 0.42043716 0.42043716
 0.42043716 0.42043716 0.42043716 0.42043716 0.42043716 0.42043716
 0.42043716 0.42043716 0.42043716 0.42043716 0.42043716 0.42043716
 0.42043716 0.42043716 0.42043716 0.42043716 0.42043716 0.42043716
 0.42043716 0.42043716 0.42043716 0.42043716 0.42043716 0.42043716
 0.42043716 0.42043716 0.42043716 0.42043716 0.42043716 0.42043716
 0.42043716 0.42043716 0.42043716 0.42043716 0.42043716 0.42043716
 0.42043716 0.42043716 0.42043716 0.42043716 0.42043716 0.42043716
 0.42043716 0.42043716 0.42043716 0.42043716 0.42043716 0.42043716
 0.42043716 0.42043716 0.42043716 0.42043716 0.42043716 0.42043716
 0.42043716 0.42043716 0.42043716 0.42043716 0.42043716 0.42043716
 0.42043716 0.42043716 0.42043716 0.42043716 0.42043716 0.4204389
 0.4204389  0.4204389  0.4204389  0.42043894 0.42043894 0.42043894
 0.42043894 0.42043894 0.42043894 0.4204392  0.4204392  0.4204392
 0.42044013 0.42044013 0.42044013 0.42044013 0.42044013 0.42044013
 0.42044013 0.42044013 0.42044013 0.42044013 0.42044013 0.42044013
 0.42044013 0.42044013 0.42044013 0.42044013 0.42044013 0.42044013
 0.42044013 0.42044013 0.42044013 0.42044013 0.42044013 0.42044013
 0.42044018 0.42044018 0.42044024 0.42044024 0.42044024 0.42044035
 0.42044035 0.42044035 0.42044035 0.42044035 0.42044035 0.42044035
 0.42044035 0.42044035 0.42044035 0.42044035 0.42044035 0.42044035
 0.42044035 0.42044035 0.42044035 0.42044035 0.42044035 0.42044035
 0.42044035 0.42044035 0.42044035 0.42044035 0.42044035 0.42044061
 0.42044061 0.42044061 0.42044061 0.42044061 0.42044061 0.42044061
 0.42044061 0.42044061 0.42044061 0.42044061 0.42044061 0.42044061
 0.42044125 0.42044125 0.42044147 0.42044345 0.42044345 0.42044345
 0.42044345 0.42044345 0.42044345 0.42044352 0.42044385 0.42044385
 0.42044385 0.42044385 0.42044385 0.42044385 0.42044385 0.42044385
 0.42044385 0.42044385 0.42044385 0.42044385 0.42044385 0.42044385
 0.42044385 0.42044385 0.42044385 0.42044385 0.42044385 0.42044385
 0.42044385 0.42044385 0.42044385 0.42044385 0.42044385 0.42044385
 0.42044385 0.42044385 0.42044385 0.42044385 0.42044385 0.42044385
 0.42044385 0.42044385 0.42044385 0.42044385 0.42044385 0.42044385
 0.42044385 0.42044385 0.42044385 0.42044385 0.42044385 0.42044385
 0.42044385 0.42044385 0.42044385 0.42044385 0.42044385 0.42044385
 0.42044385 0.42044385 0.42044385 0.42044385 0.42044385 0.42044385
 0.42044385 0.42044385 0.42044385 0.42044385 0.42044385 0.42044385
 0.42044385 0.42044385 0.42044385 0.42044385 0.4204449  0.4204449
 0.4204449  0.42044901 0.42044901 0.42044901 0.42044901 0.42044901
 0.42044901 0.42044901 0.42044972 0.42044972 0.42044972 0.42044972
 0.42044972 0.42044972 0.42044972 0.42044972 0.42044972 0.42044972
 0.42044972 0.42044972 0.42045125 0.42045125 0.42045125 0.42045297
 0.42045374 0.42045431 0.42045597 0.42045597 0.42045597 0.42045597
 0.42045597 0.42045597 0.42045597 0.42045971 0.42046018 0.42046123
 0.42046123 0.42046123 0.42046123 0.42046123 0.42046123 0.42046268
 0.42046268 0.42046268 0.42046268 0.42046268 0.42046268 0.42046268
 0.42046268 0.42046268 0.42046268 0.42046268 0.42046268 0.42046268
 0.42046268 0.42046268 0.42046268 0.42046268 0.42046268 0.42046268
 0.42046268 0.42046268 0.42046268 0.42046268 0.42046268 0.42046268
 0.42046268 0.42046268 0.42046268 0.42046268 0.42046268 0.42046268
 0.42046268 0.42046268 0.42046268 0.42046268 0.42046268 0.42046268
 0.42046268 0.42046268 0.42046268 0.42046379 0.42046379 0.42046379
 0.42046379 0.42046379 0.42046472 0.42046472 0.42046472 0.42046472
 0.42046472 0.42046896 0.42046896 0.42046896 0.42046896 0.42046996
 0.42046996 0.42046996 0.42047006 0.42047006 0.42047006 0.42047006
 0.42047006 0.42047006 0.42047006 0.42047006 0.42047006 0.42047006
 0.42047006 0.42047006 0.42047006 0.42047006 0.42047006 0.42047006
 0.42047006 0.42047006 0.42047006 0.42047006 0.42047006 0.42047147
 0.42047147 0.42047686 0.42047686 0.42047686 0.42047686 0.42047686
 0.42047686 0.42047686 0.42047686 0.42047686 0.42047686 0.42047686
 0.42047686 0.42047686 0.42047829 0.42047829 0.42047829 0.42047829
 0.42047829 0.42047829 0.42047829 0.42047829 0.42047829 0.42047829
 0.42047829 0.42047829 0.42047829 0.42047829 0.42047829 0.42047829
 0.42047829 0.42047829 0.42047829 0.42047829 0.42047829 0.42047829
 0.42047829 0.42047829 0.42047829 0.42047829 0.42047829 0.42047829
 0.42047829 0.42047829 0.42047829 0.42047829 0.42047829 0.42047829
 0.42047829 0.42047829 0.42047829 0.42047829 0.42047829 0.42047829
 0.42047829 0.42047829 0.42047829 0.4204868  0.4204868  0.4204868
 0.4204868  0.4204868  0.4204868  0.4204868  0.4204868  0.4204868
 0.4204868  0.4204868  0.4204868  0.4204868  0.4204868  0.4204868
 0.4204868  0.4204868  0.4204868  0.4204868  0.4204868  0.4204868
 0.4204868  0.4204868  0.4204868  0.4204868  0.4204868  0.4204868
 0.4204868  0.4204868  0.4204868  0.4204868  0.4204868  0.4204868
 0.4204868  0.42048833 0.42048833 0.42048833 0.42048833 0.42048833
 0.42048833 0.42048833 0.42048833 0.42048833 0.42048833 0.42048833
 0.42048833 0.42048833 0.42048833 0.42048833 0.42048833 0.42048833
 0.42048833 0.42048833 0.42048833 0.42048833 0.42048833 0.42048833
 0.42048833 0.42048833 0.42048833 0.42049265 0.42049265 0.42049265
 0.42049265 0.42049265 0.42049265 0.42049265 0.42049265 0.42049265
 0.42049265 0.42049265 0.42049265 0.42049265 0.42049265 0.42049265
 0.42049265 0.42049265 0.42049265 0.42049275 0.42049275 0.42049275
 0.42049275 0.42049491 0.42049491 0.4204959  0.4204959  0.4204959
 0.4204959  0.4204959  0.4204973  0.4204973  0.4204973  0.42049836
 0.42049836 0.42050431 0.42050431 0.42050431 0.42050431 0.42050431
 0.42050431 0.42050431 0.42050431 0.42050431 0.42050431 0.42050431
 0.42050431 0.42050431 0.42050431 0.42050431 0.42050431 0.42050431
 0.42050431 0.42050431 0.42050431 0.42050431 0.42050431 0.42050431
 0.42050943 0.42050943 0.42050943 0.42050943 0.42050943 0.42050943
 0.42050968 0.42050968 0.42050968 0.42050968 0.42050968 0.42050968
 0.42050968 0.42050968 0.42050968 0.42050968 0.42050968 0.42050968
 0.42050968 0.42050968 0.42050968 0.42050968 0.42050968 0.42050968
 0.42050968 0.42050968 0.42050968 0.42050968 0.42050968 0.42050968
 0.42050968 0.42050968 0.42050968 0.42050968 0.42050968 0.42050968
 0.42050968 0.42050968 0.42050968 0.42050968 0.42050968 0.42050968
 0.42050968 0.42050968 0.42050968 0.42050968 0.42050968 0.42050968
 0.42050968 0.42050968 0.42050968 0.4205171  0.4205171  0.4205171
 0.4205171  0.4205171  0.4205171  0.4205171  0.4205171  0.4205171
 0.4205171  0.4205171  0.4205171  0.4205171  0.4205171  0.4205171
 0.4205171  0.4205171  0.4205171  0.4205171  0.4205171  0.4205171
 0.4205171  0.4205171  0.4205171  0.4205171  0.4205171  0.4205171
 0.4205171  0.4205171  0.4205171  0.4205171  0.4205171  0.4205171
 0.4205171  0.4205171  0.4205171  0.4205171  0.4205171  0.4205171
 0.4205171  0.4205171  0.4205171  0.4205171  0.4205171  0.4205171
 0.4205171  0.4205171  0.4205171  0.4205171  0.42051773 0.42052518
 0.42053305 0.42054883 0.42054883 0.42054883 0.42054883 0.42054883
 0.42054883 0.42054883 0.42054883 0.42054883 0.42054883 0.42054883
 0.42054883 0.42054883 0.42054883 0.42054883 0.42054883 0.42054883
 0.42057853 0.42058236 0.42058236 0.42058336 0.42058336 0.42058336
 0.42058336 0.42058336 0.42058336 0.42058336 0.42058336 0.42058336
 0.42058336 0.42058336 0.42058336 0.42058336 0.42058336 0.42058336
 0.42058336 0.42058336 0.42058336 0.42058336 0.42058336 0.42058336
 0.42058336 0.42058336 0.42058336 0.42058336 0.42058336 0.42058336
 0.42058336 0.42065374 0.42069771 0.42069771 0.42069771 0.42069771
 0.42069771 0.42069771 0.42069771 0.42087999]
Check training accuracy is:  0.8009480661129289
[Find cached wc_errors, chosen_wcs.pkl loading...]
40
min_error_of_wc 0.4178030636641345
wc_index 7567
training error is:  0.1958637469586375
90.164856 seconds for one epoch
min_error_of_wc 0.3854806555798121
wc_index 6055
training error is:  0.18797717929356494
91.110011 seconds for one epoch
min_error_of_wc 0.41302831092116143
wc_index 4460
training error is:  0.18835472774561623
90.035021 seconds for one epoch
min_error_of_wc 0.3942954433872853
wc_index 7126
training error is:  0.17824481919624124
89.477515 seconds for one epoch
min_error_of_wc 0.39770092563864534
wc_index 3697
training error is:  0.17518248175182483
90.420133 seconds for one epoch
min_error_of_wc 0.4127151119076617
wc_index 5392
training error is:  0.17094554912324855
90.506684 seconds for one epoch
min_error_of_wc 0.42468360698959384
wc_index 7268
training error is:  0.16448527561036996
90.264181 seconds for one epoch
min_error_of_wc 0.40022371001962664
wc_index 232
training error is:  0.1632267807701988
90.053288 seconds for one epoch
min_error_of_wc 0.4249393019636193
wc_index 1905
training error is:  0.16012249349777663
91.128370 seconds for one epoch
min_error_of_wc 0.41410184833066793
wc_index 7095
training error is:  0.15596946052521188
90.289740 seconds for one epoch
min_error_of_wc 0.4041811364134852
wc_index 3457
training error is:  0.15122912996056714
90.138628 seconds for one epoch
min_error_of_wc 0.4271755841284421
wc_index 3284
training error is:  0.15156472858461278
90.837670 seconds for one epoch
min_error_of_wc 0.41061528944501596
wc_index 6729
training error is:  0.14451715747965432
89.964068 seconds for one epoch
min_error_of_wc 0.4130961231952096
wc_index 188
training error is:  0.14321671281147752
89.736811 seconds for one epoch
min_error_of_wc 0.42814217799863497
wc_index 1021
training error is:  0.14174846883127779
90.877685 seconds for one epoch
min_error_of_wc 0.4239951847265888
wc_index 2340
training error is:  0.1349945465223592
90.792969 seconds for one epoch
min_error_of_wc 0.42086556363890915
wc_index 1503
training error is:  0.1352881953183992
90.469879 seconds for one epoch
min_error_of_wc 0.4277935378609334
wc_index 127
training error is:  0.1340716503062337
90.363320 seconds for one epoch
min_error_of_wc 0.4254283720396561
wc_index 843
training error is:  0.1288279218055206
90.606618 seconds for one epoch
min_error_of_wc 0.42785037282269694
wc_index 3506
training error is:  0.12857622283748638
91.086406 seconds for one epoch
min_error_of_wc 0.43337206217801594
wc_index 6652
training error is:  0.12824062421344073
90.822459 seconds for one epoch
min_error_of_wc 0.43178170098146657
wc_index 3419
training error is:  0.12815672455742932
89.948431 seconds for one epoch
min_error_of_wc 0.4370326842754504
wc_index 7239
training error is:  0.123500293648796
91.333319 seconds for one epoch
min_error_of_wc 0.4150007046394109
wc_index 9866
training error is:  0.1255558352210756
90.836273 seconds for one epoch
min_error_of_wc 0.4401036464439001
wc_index 3842
training error is:  0.12232569846463626
90.235717 seconds for one epoch
min_error_of_wc 0.4367067342171207
wc_index 4463
training error is:  0.12014430740833959
90.922515 seconds for one epoch
min_error_of_wc 0.4216468895180468
wc_index 7095
training error is:  0.11573957546774061
90.645791 seconds for one epoch
min_error_of_wc 0.413148495952986
wc_index 3649
training error is:  0.11590737477976343
91.156857 seconds for one epoch
min_error_of_wc 0.43831928556646377
wc_index 8522
training error is:  0.11607517409178625
90.686213 seconds for one epoch
min_error_of_wc 0.4298856799374824
wc_index 5415
training error is:  0.11049584696702741
90.721996 seconds for one epoch
min_error_of_wc 0.41467872243252957
wc_index 9969
training error is:  0.11011829851497612
90.727157 seconds for one epoch
min_error_of_wc 0.4441459391797573
wc_index 8279
training error is:  0.10776910814665663
91.426748 seconds for one epoch
min_error_of_wc 0.4378110954181436
wc_index 8287
training error is:  0.10894370333081638
90.937018 seconds for one epoch
min_error_of_wc 0.43298392058957696
wc_index 7421
training error is:  0.10349022569007471
90.586720 seconds for one epoch
min_error_of_wc 0.41971022793234863
wc_index 4629
training error is:  0.10344827586206895
90.753101 seconds for one epoch
min_error_of_wc 0.4380895634858943
wc_index 6161
training error is:  0.10084738652571523
89.780827 seconds for one epoch
min_error_of_wc 0.4295204765744752
wc_index 3467
training error is:  0.10244147998993203
91.255326 seconds for one epoch
min_error_of_wc 0.4409545651426714
wc_index 8324
training error is:  0.10164443325782369
90.304400 seconds for one epoch
min_error_of_wc 0.44311583835867097
wc_index 3384
training error is:  0.09887574460944715
90.559362 seconds for one epoch
min_error_of_wc 0.4250165752301538
wc_index 536
training error is:  0.0985401459854015
91.162918 seconds for one epoch
np.sort(wc_errors[0:1000]) [0.42501658 0.42665743 0.42862274 0.42949903 0.42991049 0.43427593
 0.43436856 0.43441879 0.43478458 0.43478799 0.43538484 0.43599294
 0.43613596 0.43643256 0.43664628 0.43668086 0.43671019 0.43688053
 0.43704543 0.43707737 0.43711821 0.43720521 0.43740594 0.43745438
 0.43752344 0.43759104 0.43815007 0.43815518 0.43841629 0.43844359
 0.43844423 0.43862436 0.43870847 0.43872505 0.43877712 0.43878421
 0.43919478 0.4392842  0.43942559 0.43945471 0.43949786 0.43951016
 0.43974411 0.43978447 0.43992689 0.44014378 0.44018377 0.44023162
 0.44024296 0.44036152 0.44038625 0.44042096 0.44044913 0.440717
 0.44074647 0.44075883 0.44077888 0.44085711 0.44089793 0.44090663
 0.44104934 0.44105196 0.44105669 0.44122812 0.4413962  0.44140034
 0.44146141 0.44148507 0.44155025 0.44158123 0.44159343 0.44160403
 0.44163993 0.44165256 0.44166774 0.44172174 0.44178905 0.44180975
 0.4418372  0.44192859 0.44201591 0.44213732 0.44214118 0.44214883
 0.44218359 0.44226111 0.4423816  0.44240928 0.44263836 0.4426667
 0.44267253 0.4426736  0.44273769 0.44281025 0.44294439 0.44300714
 0.44304067 0.44311723 0.4431723  0.44317713 0.44319531 0.44319746
 0.4432118  0.44321193 0.4432732  0.44329136 0.44340822 0.44341242
 0.44342466 0.44346496 0.44351565 0.44354931 0.44355364 0.44357527
 0.44361404 0.44369962 0.44372945 0.44377135 0.44383674 0.44384093
 0.44384598 0.44388196 0.44395016 0.44397305 0.4439805  0.44398572
 0.44403554 0.44411576 0.44415448 0.44424939 0.44429502 0.44435136
 0.44435273 0.4443749  0.44443745 0.44444463 0.44445185 0.44448937
 0.44449462 0.44456037 0.4446117  0.44474592 0.44475333 0.44481161
 0.4448318  0.44488144 0.44495554 0.44500184 0.44503312 0.44511891
 0.44520213 0.44524769 0.44537423 0.44537655 0.44537835 0.44540424
 0.44545396 0.44554467 0.44561015 0.44562209 0.44568343 0.44580655
 0.44581735 0.44591725 0.44594079 0.44595074 0.44599761 0.44602023
 0.44604989 0.44619637 0.44621901 0.44624957 0.44625175 0.44632218
 0.44636937 0.44644189 0.44644302 0.4465107  0.4465323  0.4465539
 0.4466669  0.44667458 0.44671215 0.44677569 0.44677598 0.44677844
 0.44679827 0.4469163  0.44693718 0.44695456 0.44696027 0.44698958
 0.44704823 0.44709462 0.44716041 0.44717642 0.44718225 0.4471954
 0.44721773 0.4473009  0.4473113  0.44739212 0.44739274 0.4474602
 0.44747916 0.44747932 0.44750436 0.44753697 0.4475476  0.44759314
 0.44767134 0.44779289 0.44781318 0.44781864 0.44782176 0.44785317
 0.44787159 0.44788133 0.44797916 0.44812502 0.44813207 0.44815393
 0.44815457 0.4481857  0.44819642 0.44822077 0.44822115 0.44826332
 0.44826799 0.44827042 0.44830765 0.44832537 0.44833184 0.44836643
 0.4484171  0.4484334  0.44846688 0.44850911 0.44852971 0.44854902
 0.44856548 0.44858817 0.44861577 0.44863189 0.44867753 0.44871409
 0.44874155 0.44874932 0.44879058 0.44879767 0.44882082 0.44885047
 0.44886868 0.44888299 0.44891238 0.44897563 0.44904888 0.44908372
 0.44918699 0.44920044 0.44920445 0.44922547 0.44924647 0.44926839
 0.44933304 0.44936533 0.44942066 0.44945442 0.44946241 0.44946875
 0.44947632 0.44949332 0.44950539 0.44951121 0.44952351 0.44953043
 0.44960615 0.44968906 0.44975223 0.44978807 0.44983368 0.44984625
 0.4498833  0.44988896 0.44988993 0.4499007  0.44992422 0.44993491
 0.44999055 0.45015509 0.45017982 0.45019289 0.45036325 0.45036859
 0.45050427 0.45052354 0.45060325 0.45061401 0.45081859 0.4509221
 0.45096235 0.45096483 0.45103927 0.45108754 0.45112245 0.45115751
 0.45118405 0.45121278 0.45122967 0.45125236 0.45127471 0.45127732
 0.45129721 0.45130229 0.45133951 0.45140494 0.45143346 0.4514801
 0.45157776 0.45160105 0.45161789 0.45163854 0.45164219 0.45164561
 0.45164672 0.45167888 0.45177197 0.45191035 0.45191406 0.45199636
 0.45207774 0.4520837  0.45219432 0.45221307 0.4522622  0.45234038
 0.45234153 0.45238186 0.4524086  0.45244902 0.45255981 0.45274398
 0.45276676 0.45279408 0.45281373 0.45286053 0.45286726 0.45294341
 0.45294781 0.45298925 0.45301511 0.45303854 0.45305651 0.4530654
 0.45308682 0.45317751 0.45320242 0.45321146 0.45322815 0.45330762
 0.45331963 0.45332996 0.45336648 0.45337654 0.45341017 0.45341221
 0.45343686 0.45348684 0.45349065 0.45352456 0.4535351  0.45355909
 0.45356702 0.45373992 0.45375055 0.45383283 0.45400893 0.4540093
 0.45410648 0.45411798 0.45413836 0.45414384 0.45416017 0.45419212
 0.45422841 0.45423868 0.45424243 0.45432949 0.45434092 0.45436807
 0.45442874 0.45446028 0.4544735  0.45449557 0.45450558 0.45452025
 0.4545498  0.45463381 0.45464274 0.45469727 0.45474113 0.45476178
 0.45477533 0.45484568 0.45488542 0.45492538 0.45493094 0.45495673
 0.45497258 0.45504081 0.45504678 0.45511412 0.45512717 0.4551635
 0.45517    0.45517407 0.45518456 0.45521794 0.45525932 0.45528032
 0.45528832 0.45531    0.45531484 0.45532625 0.45532765 0.45535263
 0.45536856 0.45539029 0.45540862 0.45544908 0.45546187 0.45555279
 0.4556111  0.45562214 0.45564723 0.45572901 0.45573309 0.45581296
 0.45588105 0.45593327 0.45593514 0.45595435 0.45605185 0.45607048
 0.4561339  0.45616868 0.45621357 0.45622724 0.45625376 0.45630244
 0.45630554 0.45632518 0.45633394 0.45636186 0.45639818 0.45642019
 0.45642543 0.45642721 0.45645016 0.45661575 0.45673041 0.45676376
 0.45681167 0.4568524  0.45685934 0.45690566 0.45694472 0.45695168
 0.45701511 0.45703062 0.45707598 0.4570931  0.45709857 0.4571892
 0.45719608 0.45721189 0.45721318 0.45722245 0.45722676 0.45722839
 0.45725003 0.45726063 0.45738989 0.45740515 0.45751733 0.45758754
 0.45760421 0.45763069 0.45763768 0.45764457 0.45764506 0.45771336
 0.45772402 0.45772532 0.45776521 0.45779466 0.45786574 0.45791978
 0.4579341  0.4579578  0.45797114 0.45798889 0.45801193 0.45804125
 0.45804355 0.45805025 0.45806332 0.45808151 0.45810659 0.45820215
 0.45825079 0.458268   0.45829074 0.45829814 0.45835816 0.45836124
 0.45838871 0.45841183 0.45848231 0.45848916 0.45849885 0.45858351
 0.45867107 0.45873559 0.45875144 0.45876578 0.45878557 0.45880594
 0.45881252 0.45882435 0.45886151 0.45889214 0.45893259 0.45893401
 0.45896995 0.45898091 0.45899498 0.45905775 0.45911858 0.45913438
 0.45921189 0.4592187  0.45926561 0.45929344 0.45934463 0.45941197
 0.45941905 0.45943208 0.4595549  0.45964385 0.45965206 0.45966194
 0.45967588 0.45967837 0.45968376 0.45969308 0.45970071 0.45974717
 0.45975315 0.45975679 0.45984539 0.45985787 0.45988383 0.45990694
 0.45997446 0.4599756  0.46003386 0.4601058  0.46014181 0.46014297
 0.46022273 0.4603075  0.46031721 0.46034618 0.46034835 0.4603593
 0.46037236 0.46040885 0.46041832 0.46046832 0.4605654  0.46058871
 0.46070107 0.46070131 0.4607341  0.46073639 0.46089072 0.4609039
 0.46091693 0.46095068 0.46098978 0.46101211 0.46104424 0.46105145
 0.46108287 0.4611029  0.46111547 0.46113457 0.46115205 0.46122064
 0.46123416 0.461384   0.46142871 0.46148186 0.46153039 0.46154709
 0.46166815 0.46174278 0.46175013 0.4617514  0.46176089 0.46178927
 0.46181324 0.4618311  0.46190655 0.46203559 0.46206272 0.46207886
 0.46212453 0.46214529 0.46218714 0.46220916 0.46223486 0.4623041
 0.46230908 0.46231113 0.46239209 0.46239246 0.46240923 0.46240959
 0.46241254 0.46250437 0.46250817 0.46255528 0.46260586 0.46271697
 0.46271802 0.4627712  0.46279865 0.46280889 0.46282663 0.46282713
 0.46283631 0.46287495 0.46288356 0.46293816 0.46294391 0.46299927
 0.46301995 0.46316802 0.46319525 0.46329154 0.46329367 0.46330427
 0.46333169 0.46341532 0.46344001 0.46344406 0.46346782 0.46351133
 0.46355716 0.46356071 0.46362943 0.46363488 0.46363896 0.46371759
 0.46374223 0.46384057 0.46396455 0.46398232 0.46408686 0.46410949
 0.46412461 0.46415683 0.46416787 0.46416815 0.46420292 0.46426556
 0.46438445 0.46445219 0.46447415 0.46449438 0.46456824 0.46461243
 0.46462461 0.46464037 0.4647391  0.46491184 0.46498731 0.46505119
 0.46507867 0.46515799 0.46519577 0.46533118 0.46539402 0.4654564
 0.465552   0.46555769 0.46559704 0.4656715  0.46581307 0.46582144
 0.46585194 0.46591626 0.4659625  0.46606995 0.46610328 0.46618976
 0.46630222 0.4663161  0.46641955 0.46642078 0.46642175 0.4664238
 0.46644082 0.46646408 0.46662454 0.46668141 0.46670339 0.46674723
 0.46677146 0.46678351 0.46690981 0.46692175 0.46692825 0.4669677
 0.46699996 0.46702299 0.46704833 0.46706049 0.46716833 0.46718785
 0.46721981 0.46742761 0.46743663 0.46754612 0.46755888 0.46757375
 0.46759041 0.46761682 0.46770827 0.46773411 0.46779565 0.46783733
 0.46783789 0.46785706 0.46790172 0.46791116 0.46793053 0.46795872
 0.46798503 0.46804091 0.46809237 0.46810654 0.46812652 0.46814569
 0.46823268 0.46823287 0.46824129 0.4682858  0.46833364 0.46843033
 0.46845246 0.46845987 0.46852425 0.46852604 0.46857132 0.46859808
 0.46864492 0.46868058 0.46869846 0.46870516 0.46877741 0.46878437
 0.46880018 0.46882761 0.46892182 0.4689953  0.46905127 0.46905459
 0.46906461 0.46906506 0.46917725 0.46919694 0.46921719 0.46926324
 0.4692772  0.46930094 0.46936101 0.46936995 0.4693719  0.46937808
 0.46952319 0.46952612 0.46954724 0.46956324 0.46957145 0.46962505
 0.46965834 0.46966097 0.4696768  0.46968911 0.46972025 0.46978376
 0.46984088 0.46985942 0.46991047 0.46998984 0.47000906 0.47005479
 0.47006744 0.47006845 0.47008267 0.47008645 0.47010632 0.47010675
 0.4702165  0.47022863 0.47023377 0.47024218 0.47029943 0.47031799
 0.47040801 0.47048833 0.47049028 0.47049308 0.47062059 0.47064199
 0.47066108 0.47066319 0.47066389 0.47067238 0.47071676 0.47073071
 0.47074774 0.47075099 0.47079994 0.47080768 0.47087108 0.47088053
 0.47091869 0.47098802 0.47099314 0.4710966  0.47119326 0.47121217
 0.47122272 0.47127139 0.4713532  0.47140065 0.47152051 0.47153679
 0.47155618 0.47172103 0.47172181 0.47183049 0.47198003 0.4722747
 0.47234051 0.47235442 0.4723874  0.47241115 0.47246893 0.47250109
 0.47255949 0.47266194 0.47272159 0.47274141 0.47278497 0.47290759
 0.47295634 0.4729763  0.47314285 0.47324334 0.473338   0.4734129
 0.47344293 0.47362308 0.47367294 0.47377683 0.47379421 0.47385412
 0.4743423  0.47439515 0.47445906 0.47449752 0.47455996 0.47474607
 0.47491302 0.47495617 0.4749722  0.47512734 0.47517835 0.47518844
 0.4754782  0.47557755 0.47562155 0.47565333 0.47571209 0.47589579
 0.47597499 0.47605772 0.47631455 0.47645645 0.47652255 0.47655983
 0.47656078 0.47657306 0.47664942 0.47673679 0.47702019 0.47704336
 0.47726973 0.47742198 0.47761847 0.47763729 0.47768898 0.47770849
 0.47783072 0.47807747 0.47816532 0.47839964 0.47854575 0.47875575
 0.47887035 0.47900964 0.47902417 0.47904704 0.47905639 0.47911655
 0.47914481 0.47928727 0.47944952 0.47958101 0.47980748 0.47982376
 0.4798317  0.47989201 0.4799404  0.47998203 0.48014959 0.48020679
 0.48031618 0.48036969 0.48042456 0.48073392 0.48088392 0.48089076
 0.48097205 0.48102106 0.48190872 0.48215743 0.48220103 0.48230514
 0.48241279 0.48278542 0.48284861 0.4830257  0.48307854 0.4833637
 0.48338528 0.48341775 0.48347014 0.48356968 0.48365787 0.48388294
 0.48392941 0.48393812 0.48410602 0.48422608 0.4843679  0.4843907
 0.4843912  0.48444246 0.48456061 0.48469954 0.4847203  0.48475072
 0.48475677 0.48481916 0.48492868 0.48494003 0.48512358 0.48516118
 0.48526128 0.48529408 0.48530503 0.48536975 0.48539078 0.48551363
 0.48558711 0.48572604 0.48578289 0.48585714 0.48595841 0.48596579
 0.48608327 0.48665582 0.48676628 0.48742293]
Check training accuracy is:  0.9014598540145985
[Find cached wc_errors, chosen_wcs.pkl loading...]
50
min_error_of_wc 0.43680765802714333
wc_index 2888
training error is:  0.0960231563050592
89.326960 seconds for one epoch
min_error_of_wc 0.43696996580906067
wc_index 8416
training error is:  0.09791089856531587
89.234514 seconds for one epoch
min_error_of_wc 0.4475571201955311
wc_index 65
training error is:  0.09757529994127023
88.840603 seconds for one epoch
min_error_of_wc 0.44122761565158297
wc_index 1800
training error is:  0.09656850406913331
89.092139 seconds for one epoch
min_error_of_wc 0.43969091341558
wc_index 7629
training error is:  0.09354811645272254
88.952327 seconds for one epoch
min_error_of_wc 0.4210647382668449
wc_index 4926
training error is:  0.09296081886064267
89.800978 seconds for one epoch
min_error_of_wc 0.44368340159508185
wc_index 3867
training error is:  0.09207987247252292
90.591473 seconds for one epoch
min_error_of_wc 0.43485144242560253
wc_index 7182
training error is:  0.08863998657605499
89.620156 seconds for one epoch
min_error_of_wc 0.42352511014364913
wc_index 8407
training error is:  0.08863998657605499
89.365958 seconds for one epoch
min_error_of_wc 0.44475215241881344
wc_index 4751
training error is:  0.0888077858880778
89.793811 seconds for one epoch
min_error_of_wc 0.4436792185975746
wc_index 5018
training error is:  0.0891014346841178
89.814215 seconds for one epoch
min_error_of_wc 0.44456476319274624
wc_index 5718
training error is:  0.08662639483178114
90.043432 seconds for one epoch
min_error_of_wc 0.4294474176461936
wc_index 4170
training error is:  0.0855356993036328
89.793252 seconds for one epoch
min_error_of_wc 0.4481285622060207
wc_index 1900
training error is:  0.08503230136756434
89.985965 seconds for one epoch
min_error_of_wc 0.4469761054115935
wc_index 19
training error is:  0.08440305394747882
89.549862 seconds for one epoch
min_error_of_wc 0.4473163717621753
wc_index 404
training error is:  0.08427720446346165
90.330340 seconds for one epoch
min_error_of_wc 0.4458222419169686
wc_index 8433
training error is:  0.08419330480745024
90.827646 seconds for one epoch
min_error_of_wc 0.43278534859587847
wc_index 6866
training error is:  0.08083731856699383
90.519459 seconds for one epoch
min_error_of_wc 0.4313826268115181
wc_index 3432
training error is:  0.08205386357915934
90.880182 seconds for one epoch
min_error_of_wc 0.45085924904600194
wc_index 3305
training error is:  0.08066951925497101
90.199418 seconds for one epoch
min_error_of_wc 0.43656388547135583
wc_index 3328
training error is:  0.0813407165030623
90.733148 seconds for one epoch
min_error_of_wc 0.4494666423996042
wc_index 45
training error is:  0.08188606426713652
89.888290 seconds for one epoch
min_error_of_wc 0.45396487319925827
wc_index 5961
training error is:  0.07978857286685126
90.660019 seconds for one epoch
min_error_of_wc 0.42486991200246543
wc_index 970
training error is:  0.08016612131890255
91.113964 seconds for one epoch
min_error_of_wc 0.4467158969845981
wc_index 8195
training error is:  0.07836227871465729
90.306418 seconds for one epoch
min_error_of_wc 0.42853908039496075
wc_index 9246
training error is:  0.07672623542243473
90.633841 seconds for one epoch
min_error_of_wc 0.4492005023893811
wc_index 75
training error is:  0.0760130883463378
90.587984 seconds for one epoch
min_error_of_wc 0.4419402170120103
wc_index 61
training error is:  0.07609698800234921
90.887200 seconds for one epoch
min_error_of_wc 0.44651874143088244
wc_index 87
training error is:  0.07383169729004113
90.461418 seconds for one epoch
min_error_of_wc 0.44793904324320216
wc_index 958
training error is:  0.07341219900998408
90.415109 seconds for one epoch
min_error_of_wc 0.4505014097516125
wc_index 3416
training error is:  0.07370584780602396
91.048213 seconds for one epoch
min_error_of_wc 0.4436792459612567
wc_index 2819
training error is:  0.0723634533098414
91.029274 seconds for one epoch
min_error_of_wc 0.44377191655653037
wc_index 1805
training error is:  0.07278295158989845
91.069484 seconds for one epoch
min_error_of_wc 0.43781929367899786
wc_index 2805
training error is:  0.07055961070559613
91.687475 seconds for one epoch
min_error_of_wc 0.43520014814238517
wc_index 9958
training error is:  0.07034986156556755
91.279566 seconds for one epoch
min_error_of_wc 0.4508673488812862
wc_index 8747
training error is:  0.06925916603741922
91.314986 seconds for one epoch
min_error_of_wc 0.42802172036656255
wc_index 3611
training error is:  0.06963671448947062
91.195796 seconds for one epoch
min_error_of_wc 0.45012401638587834
wc_index 36
training error is:  0.06980451380149344
91.800068 seconds for one epoch
min_error_of_wc 0.4461233111436055
wc_index 8699
training error is:  0.06850406913331653
91.375281 seconds for one epoch
min_error_of_wc 0.4328980394545539
wc_index 9812
training error is:  0.06804262102525382
91.506961 seconds for one epoch
min_error_of_wc 0.45319789130943283
wc_index 8360
training error is:  0.06812652068126523
91.832927 seconds for one epoch
min_error_of_wc 0.451412599521491
wc_index 6516
training error is:  0.06732947394915678
91.341160 seconds for one epoch
min_error_of_wc 0.44226448981876143
wc_index 3802
training error is:  0.06737142377716254
91.840303 seconds for one epoch
min_error_of_wc 0.44840154146655575
wc_index 5687
training error is:  0.06695192549710549
92.133193 seconds for one epoch
min_error_of_wc 0.445906495470535
wc_index 4606
training error is:  0.06623877842100845
92.559433 seconds for one epoch
min_error_of_wc 0.44533084957252006
wc_index 5378
training error is:  0.0650641832368487
92.037900 seconds for one epoch
min_error_of_wc 0.45186023267968967
wc_index 130
training error is:  0.06485443409682023
91.959206 seconds for one epoch
min_error_of_wc 0.45132693511408417
wc_index 4914
training error is:  0.06485443409682023
92.616863 seconds for one epoch
min_error_of_wc 0.44543535174061716
wc_index 8747
training error is:  0.06334424028861485
92.643818 seconds for one epoch
min_error_of_wc 0.4326357312252548
wc_index 8030
training error is:  0.06233744441647793
92.631058 seconds for one epoch
np.sort(wc_errors[0:1000]) [0.43473591 0.43843235 0.43853439 0.43902258 0.43962258 0.43993147
 0.43993407 0.44004021 0.44007183 0.44010026 0.44025044 0.44034203
 0.44038784 0.44098371 0.44102623 0.44162055 0.44174078 0.44183777
 0.44214442 0.4424056  0.44254234 0.44262979 0.44290903 0.44310321
 0.44315956 0.44333116 0.44337748 0.44339855 0.44343959 0.44351069
 0.44352799 0.44359084 0.44364173 0.44364942 0.44364947 0.4437069
 0.44376495 0.44386345 0.44391832 0.44400382 0.44405286 0.44406067
 0.44408288 0.44410372 0.44413648 0.44417701 0.44419282 0.44424363
 0.44424404 0.44435074 0.44438917 0.44461051 0.44493493 0.4449792
 0.44504553 0.44521159 0.44536861 0.4455382  0.44554839 0.44573023
 0.44576757 0.44586993 0.44595591 0.44596555 0.44598892 0.4459925
 0.44599704 0.44601314 0.44602197 0.44617531 0.44624062 0.44631812
 0.44633161 0.44633223 0.44637214 0.44642075 0.44642662 0.44643309
 0.44648738 0.44657389 0.44661979 0.44674746 0.44676107 0.44680193
 0.44680384 0.44684103 0.4469276  0.44699098 0.44701273 0.44705991
 0.44716091 0.44717152 0.44721699 0.44725884 0.44728597 0.44729099
 0.44737563 0.44739771 0.44739819 0.44747109 0.4475013  0.44752782
 0.44779131 0.44784849 0.44784984 0.4478799  0.44799896 0.44803462
 0.44814362 0.44815883 0.4482401  0.44824015 0.44825584 0.44830803
 0.4483227  0.44834367 0.44840794 0.44845765 0.44846282 0.44852475
 0.44856743 0.44863966 0.44876907 0.44883662 0.44887023 0.44892103
 0.44894503 0.44898674 0.4490453  0.4491459  0.44924825 0.44941432
 0.44945321 0.44947363 0.44952471 0.44952504 0.44953725 0.44953783
 0.44954891 0.44961211 0.44969633 0.44969833 0.4497335  0.44977484
 0.4498044  0.44982839 0.44996514 0.45004793 0.4500527  0.45013062
 0.45019355 0.45024935 0.45027441 0.45035268 0.45041641 0.45049594
 0.45051817 0.45052834 0.45056109 0.45072753 0.45080221 0.45101518
 0.45102023 0.4510483  0.45105068 0.45110263 0.45120032 0.45125863
 0.45137874 0.45140642 0.45141526 0.4514328  0.45145644 0.45146879
 0.45147457 0.45147988 0.45152404 0.45152642 0.45157092 0.45163723
 0.45165779 0.45166368 0.45171527 0.45181126 0.45184981 0.45185265
 0.45194593 0.45211659 0.45212206 0.45214074 0.45223569 0.45224996
 0.45229204 0.45251226 0.45251391 0.45254792 0.45255535 0.4525557
 0.4525716  0.45263764 0.45268012 0.45270865 0.45273463 0.45285527
 0.45290397 0.45299287 0.45299692 0.45300102 0.45300669 0.45303784
 0.45304695 0.45315526 0.45319722 0.45322797 0.45323062 0.45323205
 0.45327361 0.45329102 0.45340697 0.45349403 0.45350813 0.4536646
 0.45368607 0.4536916  0.45369738 0.45369764 0.45371825 0.45373871
 0.45375204 0.45376964 0.45381535 0.45383583 0.4538941  0.45391657
 0.45402827 0.45406283 0.45413506 0.45414571 0.45422865 0.45429291
 0.45435332 0.45436148 0.45459108 0.45467776 0.45470293 0.454715
 0.45480117 0.4549027  0.45491589 0.45503032 0.45503333 0.45506087
 0.45506525 0.45508154 0.45509833 0.45516535 0.45518768 0.4553635
 0.45540063 0.4554331  0.45543399 0.45559876 0.45563583 0.45568278
 0.45569496 0.45569537 0.4557246  0.4557701  0.45577493 0.45581055
 0.4558278  0.45583846 0.45589489 0.45592696 0.45593663 0.45601117
 0.45603351 0.45611983 0.4561576  0.45616285 0.45620562 0.45623519
 0.45624043 0.45626228 0.45635448 0.45635523 0.45635632 0.45642654
 0.45647532 0.45648875 0.45666151 0.45673663 0.45676997 0.45681157
 0.45682974 0.45683385 0.45683848 0.4569611  0.45699425 0.45727183
 0.45727889 0.45728234 0.45728443 0.45729283 0.45735941 0.45739083
 0.45740112 0.45743633 0.45752817 0.45762292 0.45767455 0.45768179
 0.45769136 0.45769965 0.45770014 0.45770176 0.45771397 0.45773605
 0.45775781 0.45778116 0.45783807 0.4578734  0.45787641 0.45792261
 0.45795907 0.45796287 0.45796932 0.45800893 0.45803922 0.4580449
 0.45804556 0.45805304 0.45815716 0.4581731  0.4582174  0.4582417
 0.45832605 0.4583315  0.45834981 0.45841592 0.45846479 0.45849259
 0.45852048 0.45852679 0.4585356  0.45856404 0.45858771 0.45864463
 0.4586553  0.45877679 0.4587791  0.45878663 0.45884411 0.45891738
 0.45897655 0.45899045 0.4590335  0.45904742 0.45913117 0.45915885
 0.45916409 0.45918309 0.45918942 0.45922574 0.45927274 0.45928118
 0.4593426  0.45934435 0.45937771 0.45939222 0.45941095 0.45942455
 0.45948727 0.45953533 0.45956683 0.4596623  0.45968862 0.45972918
 0.45977058 0.4598366  0.45984686 0.45985076 0.45988597 0.45990753
 0.46000336 0.46003364 0.4600605  0.46007103 0.46009922 0.46012957
 0.46014121 0.46016936 0.46019271 0.46020209 0.46020326 0.46021949
 0.46022095 0.4602403  0.46036002 0.46036561 0.46039747 0.46040563
 0.4604144  0.4604151  0.46044624 0.4605974  0.46060918 0.46063755
 0.46068214 0.46070083 0.46078256 0.46091448 0.46092052 0.46096599
 0.46097515 0.46101151 0.46105355 0.46110276 0.46111021 0.46111092
 0.4611315  0.46118719 0.46121624 0.46125389 0.46132911 0.46137944
 0.46141307 0.46141459 0.46145336 0.46146077 0.46146168 0.46147449
 0.46149992 0.46150889 0.46152145 0.46155194 0.46162953 0.46168671
 0.46168727 0.46183388 0.46188574 0.46189107 0.46190447 0.46192558
 0.46195851 0.46200873 0.46202199 0.46205794 0.46207016 0.46214494
 0.46216745 0.46219162 0.46221326 0.46222806 0.46223737 0.4622399
 0.46224047 0.46226437 0.46227337 0.46227612 0.46229072 0.46233196
 0.46241151 0.46248323 0.46248677 0.46249359 0.46259599 0.46260596
 0.46263248 0.46265097 0.46267919 0.46277061 0.46279342 0.46279621
 0.46283152 0.46283446 0.46289049 0.46292217 0.46295778 0.46299506
 0.46301735 0.46303718 0.46316054 0.46318456 0.46325145 0.46328126
 0.46328182 0.4633442  0.46335872 0.46337466 0.46338059 0.46341165
 0.46341566 0.46345822 0.46348753 0.46349442 0.46351522 0.46351876
 0.46361176 0.46362306 0.46366419 0.46376903 0.46380202 0.4638801
 0.46389534 0.46394558 0.46397164 0.46402952 0.46409025 0.46410899
 0.46412898 0.46415278 0.46415512 0.46417519 0.46417612 0.46420378
 0.46420864 0.46421873 0.46424568 0.46433727 0.46438352 0.46439637
 0.46446311 0.46453682 0.46455956 0.46456696 0.46457183 0.46457232
 0.46459014 0.46463381 0.46463783 0.4646392  0.46466436 0.46469168
 0.46469907 0.46473704 0.46479594 0.46487624 0.46494005 0.46507418
 0.46507639 0.46508291 0.46514543 0.46515819 0.46518379 0.46519758
 0.46521378 0.46530192 0.465328   0.46535177 0.46538222 0.46546712
 0.46548711 0.46550888 0.46555669 0.46563315 0.46566083 0.46569272
 0.4657282  0.46574598 0.46575439 0.46575617 0.46576263 0.46580237
 0.46591031 0.4659444  0.46595305 0.46599856 0.46603732 0.46609019
 0.46611247 0.46615118 0.46618811 0.46620896 0.46621063 0.46622698
 0.4663408  0.46635675 0.46637718 0.4664158  0.46642114 0.46647106
 0.46647368 0.4664829  0.46651854 0.46654763 0.46655331 0.46655666
 0.46659893 0.46660783 0.46663673 0.46664803 0.46671361 0.4667851
 0.46678585 0.46680512 0.46684041 0.46684739 0.46685205 0.46687098
 0.46695054 0.46697082 0.4669956  0.46700773 0.46705196 0.46724051
 0.46733625 0.46735598 0.46737503 0.46741838 0.46748567 0.46749494
 0.4675219  0.46754816 0.46757307 0.46761582 0.46764444 0.46765378
 0.46765948 0.46768732 0.46771428 0.46776207 0.46778907 0.46779635
 0.4678019  0.4678669  0.46787822 0.46787924 0.4678958  0.46796398
 0.46801474 0.46802972 0.46807845 0.46808495 0.46817786 0.46818353
 0.46820923 0.46823503 0.46823539 0.46831426 0.46831875 0.4683285
 0.4683874  0.46841044 0.46846359 0.46846666 0.46850468 0.46860723
 0.46861027 0.4686234  0.46862777 0.46863387 0.46866416 0.4686751
 0.46868429 0.46869616 0.46882646 0.46884119 0.46886335 0.46894843
 0.46896448 0.46901826 0.46903773 0.46905776 0.4691434  0.46915185
 0.46915631 0.46916604 0.46916775 0.46924114 0.4692486  0.46931311
 0.46932049 0.46936743 0.46937628 0.46937881 0.46943422 0.46950493
 0.46954588 0.46962724 0.46963139 0.46970986 0.46973033 0.46984885
 0.46986842 0.46990007 0.46990483 0.4699066  0.4699102  0.4699367
 0.46995241 0.47002236 0.47004281 0.47004576 0.47006963 0.47012988
 0.47020604 0.47022193 0.47022985 0.4703485  0.47039418 0.47042745
 0.47051865 0.47054128 0.4705474  0.47056671 0.4705691  0.47058475
 0.47063822 0.47064041 0.47066125 0.47070761 0.47079893 0.47080348
 0.47080464 0.47089615 0.47097469 0.47099372 0.47102738 0.47105577
 0.47106935 0.47107078 0.47108922 0.47110004 0.47111187 0.47113432
 0.47113432 0.47117164 0.47122597 0.47123441 0.47123759 0.47125215
 0.47127231 0.47133848 0.47134819 0.47136741 0.47139582 0.47150518
 0.47153317 0.47160447 0.47162531 0.47162571 0.47162965 0.47164465
 0.47164637 0.47177068 0.47181793 0.47183652 0.47186    0.47186593
 0.47189219 0.47189447 0.47191467 0.4719335  0.47202398 0.47205747
 0.47208765 0.47214651 0.47216107 0.47219141 0.47227441 0.47229851
 0.47234771 0.47239606 0.47246433 0.47247527 0.47250548 0.47251947
 0.47257011 0.47262906 0.47267286 0.47269385 0.47270864 0.47273812
 0.47276568 0.47277979 0.47282623 0.47292583 0.47301675 0.47302785
 0.47305094 0.47312258 0.47312424 0.4731252  0.47314294 0.47317258
 0.47318949 0.47319545 0.47320703 0.4732764  0.47328951 0.47330545
 0.47331168 0.47335656 0.47336824 0.47347451 0.47351513 0.47352139
 0.47357287 0.47358131 0.47370106 0.47371257 0.47374685 0.47378948
 0.47383111 0.47386087 0.47390243 0.47394111 0.47398273 0.47401379
 0.47401382 0.47406715 0.47408148 0.47419953 0.47422136 0.47423294
 0.47424145 0.47427016 0.4742728  0.47429052 0.47435113 0.47441025
 0.47442827 0.47464092 0.47466845 0.47470595 0.47470605 0.47471353
 0.47472314 0.47484969 0.4748522  0.47486126 0.47488755 0.47497319
 0.47497666 0.4750264  0.47505963 0.47515624 0.47516199 0.47519824
 0.47522042 0.4752745  0.47531486 0.47531542 0.47532287 0.47534017
 0.47539812 0.47540898 0.47542199 0.47552561 0.47556982 0.4755807
 0.47560202 0.47563783 0.47564653 0.47569475 0.47569995 0.47576789
 0.47585367 0.47594596 0.47610543 0.4761322  0.47613354 0.47617871
 0.47619031 0.47626387 0.47631432 0.47632076 0.47656542 0.47660486
 0.47663865 0.4766546  0.4766973  0.47671369 0.4767428  0.47675863
 0.47678179 0.47678189 0.47684904 0.47690566 0.47693011 0.47693523
 0.47699256 0.4770019  0.47704274 0.47734371 0.47741604 0.47744495
 0.47758857 0.47760419 0.47760874 0.47764074 0.47764411 0.47765658
 0.47769298 0.47773477 0.47778205 0.47780138 0.47782756 0.47789333
 0.47792547 0.47796638 0.47797765 0.47798073 0.47798884 0.47800258
 0.47806755 0.47810563 0.47823882 0.47827282 0.47827901 0.47834052
 0.47834132 0.47840109 0.47840348 0.47840938 0.4784151  0.47844487
 0.47848573 0.47852478 0.47852874 0.47854698 0.47854856 0.47868105
 0.47870906 0.47871189 0.47871767 0.47874433 0.47876879 0.47884972
 0.47888862 0.47901291 0.47901793 0.47911917 0.47921431 0.47935277
 0.47944133 0.47952597 0.47957413 0.47960019 0.47962189 0.47970256
 0.4797733  0.4798148  0.47982726 0.47989233 0.47989353 0.47998157
 0.47998581 0.48001504 0.48005731 0.48006786 0.48010206 0.48011618
 0.4802151  0.48024587 0.48030149 0.48033201 0.48043969 0.48062742
 0.48071952 0.48074853 0.48102325 0.48103298 0.48109493 0.48130399
 0.48132555 0.48139545 0.48147211 0.48147356 0.48152778 0.48156318
 0.48171898 0.48179077 0.48228728 0.48247381 0.48250714 0.48253272
 0.4826856  0.48295776 0.48296096 0.48296943 0.48313547 0.4831453
 0.48318598 0.48321613 0.4832578  0.48342189 0.48350587 0.48364484
 0.48365137 0.48365479 0.48373205 0.48386461 0.48404793 0.48422921
 0.48424763 0.48438039 0.48472286 0.48592602]
Plotting top 20 Haar filters..
Check training accuracy is:  0.9376625555835221
t 1
t 10
t 100
t 55
alpha(voting weight) of top Haar Filter Number 0 = 0.443423
alpha(voting weight) of top Haar Filter Number 1 = 0.341916
alpha(voting weight) of top Haar Filter Number 2 = 0.284060
alpha(voting weight) of top Haar Filter Number 3 = 0.286807
alpha(voting weight) of top Haar Filter Number 4 = 0.268495
alpha(voting weight) of top Haar Filter Number 5 = 0.223424
alpha(voting weight) of top Haar Filter Number 6 = 0.199203
alpha(voting weight) of top Haar Filter Number 7 = 0.216084
alpha(voting weight) of top Haar Filter Number 8 = 0.183007
alpha(voting weight) of top Haar Filter Number 9 = 0.180883
alpha(voting weight) of top Haar Filter Number 10 = 0.165899
alpha(voting weight) of top Haar Filter Number 11 = 0.233175
alpha(voting weight) of top Haar Filter Number 12 = 0.175730
alpha(voting weight) of top Haar Filter Number 13 = 0.214646
alpha(voting weight) of top Haar Filter Number 14 = 0.207527
alpha(voting weight) of top Haar Filter Number 15 = 0.176376
alpha(voting weight) of top Haar Filter Number 16 = 0.151788
alpha(voting weight) of top Haar Filter Number 17 = 0.202267
alpha(voting weight) of top Haar Filter Number 18 = 0.151265
alpha(voting weight) of top Haar Filter Number 19 = 0.173517
Image shape is: 960 X 1280
Image shape is: 540 X 720
Image shape is: 376 X 501
Image shape is: 288 X 384
Image shape is: 234 X 311
Image shape is: 196 X 262
Image shape is: 169 X 226
Image shape is: 149 X 199
Image shape is: 133 X 177
Image shape is: 120 X 160
shape of patches 2042755
shape of patch_xyxy 2042755
shape of patches_patch_xyxy 2
Face Detection in Progress ..., total 2042755 patches
100%|| 2042755/2042755 [29:36<00:00, 1149.69it/s]
0.0043788902731849875 8945
after nms: 164
Image shape is: 960 X 1280
Image shape is: 540 X 720
Image shape is: 376 X 501
Image shape is: 288 X 384
Image shape is: 234 X 311
Image shape is: 196 X 262
Image shape is: 169 X 226
Image shape is: 149 X 199
Image shape is: 133 X 177
Image shape is: 120 X 160
shape of patches 2042755
shape of patch_xyxy 2042755
shape of patches_patch_xyxy 2
Face Detection in Progress ..., total 2042755 patches
100%|| 2042755/2042755 [29:29<00:00, 1154.12it/s]
0.004305949563212426 8796
after nms: 139
Image shape is: 960 X 1280
Image shape is: 540 X 720
Image shape is: 376 X 501
Image shape is: 288 X 384
Image shape is: 234 X 311
Image shape is: 196 X 262
Image shape is: 169 X 226
Image shape is: 149 X 199
Image shape is: 133 X 177
Image shape is: 120 X 160
  0%|          | 0/2042755 [00:00<?, ?it/s]shape of patches 2042755
shape of patch_xyxy 2042755
shape of patches_patch_xyxy 2
Face Detection in Progress ..., total 2042755 patches
100%|| 2042755/2042755 [29:30<00:00, 1154.05it/s]
0.003514371522771943 7179
after nms: 144
didnt find cached wrong patches..
Image shape is: 960 X 1280
Image shape is: 540 X 720
Image shape is: 376 X 501
Image shape is: 288 X 384
Image shape is: 234 X 311
Image shape is: 196 X 262
Image shape is: 169 X 226
Image shape is: 149 X 199
Image shape is: 133 X 177
Image shape is: 120 X 160
Get Hard Negative in Progress ..., total 2042755 patches
100%|| 2042755/2042755 [29:29<00:00, 1154.28it/s]
Number of wrong patches: (724, 5)
wrong_patches.shape (724, 5)
(724, 16, 16)
data.shape (24562, 16, 16)
labels.shape (23838,)
new labels.shape after hnm (24562,)
Calcuate activations for 10032 weak classifiers, using 24562 imags.
shape of data is (24562, 16, 16)
Weak Classifier No. 100 has finished applying
Weak Classifier No. 200 has finished applying
Weak Classifier No. 300 has finished applying
Weak Classifier No. 400 has finished applying
Weak Classifier No. 500 has finished applying
Weak Classifier No. 600 has finished applying
Weak Classifier No. 700 has finished applying
Weak Classifier No. 800 has finished applying
Weak Classifier No. 900 has finished applying
Weak Classifier No. 1000 has finished applying
Weak Classifier No. 1100 has finished applying
Weak Classifier No. 1200 has finished applying
Weak Classifier No. 1300 has finished applying
Weak Classifier No. 1400 has finished applying
Weak Classifier No. 1500 has finished applying
Weak Classifier No. 1600 has finished applying
Weak Classifier No. 1700 has finished applying
Weak Classifier No. 1800 has finished applying
Weak Classifier No. 1900 has finished applying
Weak Classifier No. 2000 has finished applying
Weak Classifier No. 2100 has finished applying
Weak Classifier No. 2200 has finished applying
Weak Classifier No. 2300 has finished applying
Weak Classifier No. 2400 has finished applying
Weak Classifier No. 2500 has finished applying
Weak Classifier No. 2600 has finished applying
Weak Classifier No. 2700 has finished applying
Weak Classifier No. 2800 has finished applying
Weak Classifier No. 2900 has finished applying
Weak Classifier No. 3000 has finished applying
Weak Classifier No. 3100 has finished applying
Weak Classifier No. 3200 has finished applying
Weak Classifier No. 3300 has finished applying
Weak Classifier No. 3400 has finished applying
Weak Classifier No. 3500 has finished applying
Weak Classifier No. 3600 has finished applying
Weak Classifier No. 3700 has finished applying
Weak Classifier No. 3800 has finished applying
Weak Classifier No. 3900 has finished applying
Weak Classifier No. 4000 has finished applying
Weak Classifier No. 4100 has finished applying
Weak Classifier No. 4200 has finished applying
Weak Classifier No. 4300 has finished applying
Weak Classifier No. 4400 has finished applying
Weak Classifier No. 4500 has finished applying
Weak Classifier No. 4600 has finished applying
Weak Classifier No. 4700 has finished applying
Weak Classifier No. 4800 has finished applying
Weak Classifier No. 4900 has finished applying
Weak Classifier No. 5000 has finished applying
Weak Classifier No. 5100 has finished applying
Weak Classifier No. 5200 has finished applying
Weak Classifier No. 5300 has finished applying
Weak Classifier No. 5400 has finished applying
Weak Classifier No. 5500 has finished applying
Weak Classifier No. 5600 has finished applying
Weak Classifier No. 5700 has finished applying
Weak Classifier No. 5800 has finished applying
Weak Classifier No. 5900 has finished applying
Weak Classifier No. 6000 has finished applying
Weak Classifier No. 6100 has finished applying
Weak Classifier No. 6200 has finished applying
Weak Classifier No. 6300 has finished applying
Weak Classifier No. 6400 has finished applying
Weak Classifier No. 6500 has finished applying
Weak Classifier No. 6600 has finished applying
Weak Classifier No. 6700 has finished applying
Weak Classifier No. 6800 has finished applying
Weak Classifier No. 6900 has finished applying
Weak Classifier No. 7000 has finished applying
Weak Classifier No. 7100 has finished applying
Weak Classifier No. 7200 has finished applying
Weak Classifier No. 7300 has finished applying
Weak Classifier No. 7400 has finished applying
Weak Classifier No. 7500 has finished applying
Weak Classifier No. 7600 has finished applying
Weak Classifier No. 7700 has finished applying
Weak Classifier No. 7800 has finished applying
Weak Classifier No. 7900 has finished applying
Weak Classifier No. 8000 has finished applying
Weak Classifier No. 8100 has finished applying
Weak Classifier No. 8200 has finished applying
Weak Classifier No. 8300 has finished applying
Weak Classifier No. 8400 has finished applying
Weak Classifier No. 8500 has finished applying
Weak Classifier No. 8600 has finished applying
Weak Classifier No. 8700 has finished applying
Weak Classifier No. 8800 has finished applying
Weak Classifier No. 8900 has finished applying
Weak Classifier No. 9000 has finished applying
Weak Classifier No. 9100 has finished applying
Weak Classifier No. 9200 has finished applying
Weak Classifier No. 9300 has finished applying
Weak Classifier No. 9400 has finished applying
Weak Classifier No. 9500 has finished applying
Weak Classifier No. 9600 has finished applying
Weak Classifier No. 9700 has finished applying
Weak Classifier No. 9800 has finished applying
Weak Classifier No. 9900 has finished applying
Weak Classifier No. 10000 has finished applying
Writing results to disk...
[Saved calculated activations to wc_activations_hnm.npy]
wc.activations shape is (10032, 24562)
wc.activations shape is (24562,)
187.341598 seconds for activation calculation
min_error_of_wc 0.28527807181826403
wc_index 87
training error is:  0.2852780718182558
108.926662 seconds for one epoch
min_error_of_wc 0.33070090915165645
wc_index 8892
training error is:  0.2849116521455908
89.334767 seconds for one epoch
min_error_of_wc 0.3595288137676164
wc_index 5383
training error is:  0.2468447194853839
88.840114 seconds for one epoch
min_error_of_wc 0.3590488027623788
wc_index 6201
training error is:  0.24297695627391902
88.126273 seconds for one epoch
min_error_of_wc 0.36898969879810406
wc_index 3429
training error is:  0.2214396221806042
89.504902 seconds for one epoch
min_error_of_wc 0.38800549351514585
wc_index 8400
training error is:  0.22380099340444592
88.707233 seconds for one epoch
min_error_of_wc 0.3960475457090485
wc_index 2150
training error is:  0.2119941372852373
88.445451 seconds for one epoch
min_error_of_wc 0.39044478947949984
wc_index 3412
training error is:  0.2028743587655728
89.365374 seconds for one epoch
min_error_of_wc 0.4081312833228192
wc_index 31
training error is:  0.199780148196401
88.508647 seconds for one epoch
min_error_of_wc 0.411105356295612
wc_index 116
training error is:  0.19346958716716878
89.123740 seconds for one epoch
min_error_of_wc 0.41657655627090706
wc_index 7567
training error is:  0.19004967022229458
89.872430 seconds for one epoch
min_error_of_wc 0.3855093878702054
wc_index 6055
training error is:  0.18300626984773227
88.917461 seconds for one epoch
min_error_of_wc 0.41105395202441863
wc_index 4460
training error is:  0.18312840973862066
89.018168 seconds for one epoch
min_error_of_wc 0.393743992236745
wc_index 7126
training error is:  0.1730315120918492
89.035636 seconds for one epoch
min_error_of_wc 0.39691313173847703
wc_index 3622
training error is:  0.169611595146975
88.593623 seconds for one epoch
min_error_of_wc 0.40947648540377146
wc_index 5392
training error is:  0.16541812555980784
87.925240 seconds for one epoch
min_error_of_wc 0.42086510516540715
wc_index 7299
training error is:  0.15910756453057573
88.930110 seconds for one epoch
min_error_of_wc 0.397691081465984
wc_index 188
training error is:  0.158089732106506
88.941184 seconds for one epoch
min_error_of_wc 0.421466271101788
wc_index 3541
training error is:  0.15458838856770618
89.019526 seconds for one epoch
min_error_of_wc 0.41673165272910045
wc_index 6977
training error is:  0.14978421952609722
89.557361 seconds for one epoch
min_error_of_wc 0.40520517473424805
wc_index 1503
training error is:  0.1466900089569253
90.415572 seconds for one epoch
min_error_of_wc 0.42823324133674534
wc_index 127
training error is:  0.14550932334500444
89.175682 seconds for one epoch
min_error_of_wc 0.42643358076451304
wc_index 7095
training error is:  0.1401758814428793
89.843365 seconds for one epoch
min_error_of_wc 0.40987774272719324
wc_index 4212
training error is:  0.1394430420975491
90.294485 seconds for one epoch
min_error_of_wc 0.4303716309311157
wc_index 154
training error is:  0.13630811823141442
89.954779 seconds for one epoch
min_error_of_wc 0.42840705322439077
wc_index 3506
training error is:  0.13516814591645632
89.300902 seconds for one epoch
min_error_of_wc 0.42345884801073097
wc_index 6729
training error is:  0.1304046901718101
90.334557 seconds for one epoch
min_error_of_wc 0.41067390143659016
wc_index 432
training error is:  0.1288575848872242
90.171954 seconds for one epoch
min_error_of_wc 0.43870141717207906
wc_index 7907
training error is:  0.1283283120267079
89.786673 seconds for one epoch
min_error_of_wc 0.4350605391110719
wc_index 3842
training error is:  0.12735119289960106
90.183683 seconds for one epoch
min_error_of_wc 0.43731108017951625
wc_index 2340
training error is:  0.12474554189398257
91.425968 seconds for one epoch
min_error_of_wc 0.43873346579556494
wc_index 17
training error is:  0.12494910837879647
89.548368 seconds for one epoch
min_error_of_wc 0.4384338303071249
wc_index 7630
training error is:  0.12189561110658742
89.780767 seconds for one epoch
min_error_of_wc 0.4126789314231994
wc_index 9638
training error is:  0.12148847813695951
94.421661 seconds for one epoch
min_error_of_wc 0.43811714415877917
wc_index 6920
training error is:  0.11758000162853188
91.858571 seconds for one epoch
min_error_of_wc 0.40973164200092965
wc_index 9990
training error is:  0.1167250223923133
90.277631 seconds for one epoch
min_error_of_wc 0.43935045093333763
wc_index 3348
training error is:  0.11497435062291339
91.041184 seconds for one epoch
min_error_of_wc 0.4369362717367917
wc_index 6156
training error is:  0.111025160817523
91.453041 seconds for one epoch
min_error_of_wc 0.4203887122787456
wc_index 3284
training error is:  0.11167657356892757
91.896738 seconds for one epoch
Traceback (most recent call last):
  File "/home/vaishnaviravindran29/PycharmProjects/prml/main.py", line 174, in <module>
    main()
  File "/home/vaishnaviravindran29/PycharmProjects/prml/main.py", line 156, in main
    boost.train(wc_activations, 100, chosen_wc_cache_dir_hnm, save_dir_sc_error_hnm, save_dir_current_weights_hnm,hnm=True)
  File "/home/vaishnaviravindran29/PycharmProjects/prml/boosting_classifier.py", line 113, in train
    in self.weak_classifiers)
  File "/home/vaishnaviravindran29/PycharmProjects/prml/venv/lib/python3.4/site-packages/joblib/parallel.py", line 930, in __call__
    self.retrieve()
  File "/home/vaishnaviravindran29/PycharmProjects/prml/venv/lib/python3.4/site-packages/joblib/parallel.py", line 833, in retrieve
    self._output.extend(job.get(timeout=self.timeout))
  File "/home/vaishnaviravindran29/PycharmProjects/prml/venv/lib/python3.4/site-packages/joblib/_parallel_backends.py", line 521, in wrap_future_result
    return future.result(timeout=timeout)
  File "/usr/lib/python3.4/concurrent/futures/_base.py", line 397, in result
    self._condition.wait(timeout)
  File "/usr/lib/python3.4/threading.py", line 290, in wait
    waiter.acquire()
KeyboardInterrupt

Process finished with exit code 1
